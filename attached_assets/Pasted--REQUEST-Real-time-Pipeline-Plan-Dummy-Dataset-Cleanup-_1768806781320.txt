[REQUEST: Real-time Pipeline Plan + Dummy Dataset Cleanup (NO CODE CHANGES)]

You are acting as an architect reviewing and preparing the system for a stable real-time waiting-data pipeline (Phase 2A/2B direction).

0) Hard Constraints

DO NOT modify any application code (server/client/shared).

This request is analysis + plan + data-file cleanup only.

If any step would require code changes, just document it as a next step — do not implement.

1) Ask: Step-by-step plan for a stable real-time pipeline

Please produce a structured plan that answers:

1.1 Target behavior (Today tab)

Today tab should show near-real-time waiting data with 30s–60s cadence.

Menus do not need real-time updates (file-based daily updates are fine).

Waiting data will eventually be sourced from a DB (PostgreSQL recommended), but we want a safe phased migration.

1.2 Phased roadmap (concrete, implementable)

Provide an ordered checklist for:

Phase 2A (shadow write): DB schema + ingestion endpoint + validation + auth + logging, but keep reads as-is.

Phase 2B (today read switch): introduce /api/waiting/latest?date=... (or equivalent) reading from DB for today only, with a feature flag fallback.

Phase 2C/2D: historical migration and cleanup (optional later).

For each phase, specify:

What changes are required (conceptually)

What can be validated without breaking the UI

Risks & mitigations (IDs, timestamp format, duplicates, partial snapshots)

Operational steps (how to run, how to verify, rollback approach)

1.3 Key design decisions to confirm

Please explicitly recommend defaults for:

Batch vs per-corner ingestion: prefer batch snapshot if viable; define “partial snapshot” semantics.

Uniqueness + upsert key: (timestamp, restaurantId, cornerId)

Timestamp format: ISO 8601 with +09:00 (KST) on output

How to compute est_wait_time_min: server-side only

What /api/dates and /api/waiting/timestamps should mean once DB exists

1.4 Verification plan

Give a practical verification checklist:

Curl examples for ingestion and read endpoints (even if not implemented yet)

What to check in logs

How to validate ID mismatches early

How to detect “no ingestion for >5min” during operating hours

Deliver this as a concise “implementation-ready” plan.

2) Request: Clean up test dummy datasets (data-only operation)

We are preparing to connect real-time ingestion, so we must remove the current test/dummy datasets from the repo runtime.

2.1 Safety requirement (backup first)

Before deleting anything:

Create a backup folder (example: data/_backup_dummy/)

Move the existing dummy files into that backup folder

Then ensure the app will still boot in “skeleton/placeholder mode”.

2.2 What to remove / neutralize

Please identify and then move (or replace with empty skeleton files) the current dummy datasets:

data/menus_by_date.json

data/hy_eat_queue_3days_combined.csv (or whichever CSV is currently used)

Any other test data files in data/ that are actively referenced by the server for menus/waiting.

2.3 Skeleton mode requirement

After cleanup:

The server should still start without crashing.

The UI should show the existing graceful placeholder states (“데이터 없음” etc.).

If the server requires the files to exist, replace them with minimal valid empty formats:

menus JSON: {} or { "YYYY-MM-DD": {} } (pick what the current loader accepts)

waiting CSV: header-only file timestamp,restaurantId,cornerId,queue_len,data_type with no rows

2.4 Output needed

Report back with:

Exactly which files were moved/changed

Their new paths

What “empty skeleton” content you used (if applicable)

A quick verification note: app boots + endpoints return empty but valid responses

3) Final deliverable format

Please reply with:

Real-time pipeline plan (Phase 2A→2B→2C)

Dummy data cleanup report (file list + actions)

No-code-change compliance confirmation